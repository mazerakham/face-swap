The app is a workflow that takes place in a React browser
application communicating with the Python server backend.
All API calls are to this server, which in turn uses other
APIs (OpenAI, Icon8, etc) for some things.  

Step 0: User lands on the app.  For now, no login.  They
just start on the first step of the workflow which is:

Step 1: User uploads a photo of themselves.  They are instructed
to make it a headshot.  When this happens, the photo is sent to
the backend which saves the photo on S3 with a public URL.  
This will be referred to as the Base Image.  This image URL is 
sent to another API for describing the user in the image.
This API returns a description of the user which is very detailed,
including racial/ethnic/gender information, but avoids talking
about the poise or background.  Only physical characteristics.

Key terms:
- Base Headshot Image
- Base User Description

Step 2: User is prompted a sequence of questions.  The user
is told to visualize themselves having accomplished their
most ambitious life goals and envision what that looks like.
The app asks them a sequence of questions (and saves the 
answers in browser context):

- What is the setting?
- What are you wearing?
- What is the emotion?

We will refer to these questions/answers as the 

Key term:
- User Questionnaire

Step 3:  The Base User Description and User Questionnaire are
sent to an image generation API.  The API returns an Image
Generation Result, which has two components: (1) a URL pointing
to the generated image, called the Current Target Image and 
(2) an Augmented Prompt which was  generated by OpenAI - it's 
the ACTUAL prompt that OpenAI used to generate the image.  
We save this because it's useful for subsequent image generations 
to use this prompt so that the results don't change too much 
from one try to another.

Key terms:
- Image Generation Result
- Augmented Prompt
- Current Target Image

Step 4: The user is asked to comment on the Current Target Image.
They can type in any feedback they have.  This is appended
to the augmented prompt as '\n\nUser Feedback: ...' and sent
back to the image generation API to generate the new image. 
Thus the things we're sending to the Image Generation API are:
1. The Augmented Prompt associated with the Current Target Image.
2. The Base User Description
3. The User Questionnaire
4. The user's feedback on the Current Target Image.
The API returns a new image which becomes the Current Target Image
and a new Augmented Prompt associated with it.  This process thus
repeates until the user indicates their satisfaction to move on.

Step 5: The Base Image is face-swapped into the Final Target Image
using my face swap API, also already implemented in the backend.
The user is presented with the Final Result.  The process is over.

*Augmented Prompts: every time an image is generated by the API,
it needs to be returned ALONG WITH an augmented prompt.  This is
crucial if the image needs to be edited later.  OpenAI's APIs do 
not support robust image editing, so the only thing we can do is
capture OpenAI/Dall-E's "augmented prompts" (where they make
adjustments to our prompt before generating the image) and use 
(and adjust) that augmented prompt when we want to make edits to
the image.  Front-end will be responsible for maintaining
a cache of images and their associated augmented prompts.  

